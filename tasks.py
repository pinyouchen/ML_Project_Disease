import os
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    roc_curve, precision_recall_curve, auc, 
    f1_score, accuracy_score, precision_score, 
    recall_score, confusion_matrix, roc_auc_score
)
from sklearn.ensemble import IsolationForest
import shap 

# å¼•ç”¨è‡ªå®šç¾©æ¨¡çµ„
from model_trainer import ModelTrainer
from utils import save_best_model, pretty_print_table, load_best_model_and_meta
from visualization import Visualizer

from processors import (
    ProcessorBaseline4
)

def run_binary_task(task_name, file_path, sheet_name, processor_cls, use_stacking=True):
    print("\n" + "="*70)
    print(f"åŸ·è¡Œä»»å‹™: {task_name} (AutoML & SHAP-OOF Version)")
    print("="*70)
    
    timestamp = datetime.now().strftime(f"{task_name}_%Y%m%d_%H%M%S")
    run_dir = os.path.join(os.getcwd(), "runs", timestamp)
    models_dir = os.path.join(run_dir, "models")
    os.makedirs(models_dir, exist_ok=True)
    
    processor = processor_cls(file_path, sheet_name)
    if not processor.load_data(): return
    if not processor.prepare_features_and_labels(): return
    
    label_names = ['SSD', 'MDD', 'Panic', 'GAD']
    y_dict = processor.y_dict
    df_full = processor.df
    X_full = processor.X
    
    # [Check] ç¢ºä¿ df_full æœ‰ Subject_IDï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨ Index ä½œç‚º ID
    if 'Subject_ID' not in df_full.columns:
        df_full['Subject_ID'] = df_full.index
    
    summary_rows = []
    overall_roc_data = {}
    overall_pr_data = {}
    overall_metrics_list = [] 
    
    for label in label_names:
        if label not in y_dict: continue
        print(f"\nğŸ©º è¨ºæ–·ï¼š{label} vs Health")
        
        mask_disease = df_full[label] == 1
        mask_health = (df_full['Health'] == 1) & (df_full[label] == 0)
        mask_valid = mask_disease | mask_health
        
        # é€™è£¡ä¿ç•™åŸå§‹ Indexï¼Œé€™å° Step 4 è‡³é—œé‡è¦
        X_sub = X_full.loc[mask_valid].copy()
        y_sub = np.where(mask_disease[mask_valid], 1, 0)
        
        viz = Visualizer(label, run_dir, sub_folder=label)

        print("   ğŸ“Š ç¹ªè£½ç‰¹å¾µç›¸é—œæ€§çŸ©é™£ (EDA)...")
        try:
            X_sub_corr_p = processor.impute_and_scale(X_sub, fit=True)
            viz.plot_correlation_matrix(X_sub_corr_p)
        except Exception as e:
            print(f"   âš ï¸ ç„¡æ³•ç¹ªè£½ Correlation Matrix: {e}")

        base_f1 = {'SSD':0.66, 'MDD':0.46, 'Panic':0.50, 'GAD':0.57}.get(label, 0.5)
        target_f1 = {'SSD':0.75, 'MDD':0.75, 'Panic':0.55, 'GAD':0.70}.get(label, 0.7)
        
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        metrics_list = []
        tprs = []; mean_fpr = np.linspace(0, 1, 100)
        roc_aucs = []
        precisions = []; mean_recall = np.linspace(0, 1, 100)
        pr_aucs = []
        no_skill = y_sub.sum() / len(y_sub)
        y_true_all = []; y_pred_all = []
        shap_values_list = []
        X_test_shap_list = []
        importance_list = []
        best_model_info = {"f1": -1.0, "p": -1.0, "obj": None, "name": None}
        
        # [æ–°å¢] ç”¨æ–¼å„²å­˜ Step 4 æ‰€éœ€çš„è©³ç´°é æ¸¬è³‡æ–™ (Out-Of-Fold Predictions)
        oof_predictions_list = []

        fold_id = 1
        for train_idx, test_idx in skf.split(X_sub, y_sub):
            print(f"\n   ğŸ“‚ Fold {fold_id}/5")
            
            # [æ–°å¢] å–å¾—é€™ä¸€æŠ˜æ¸¬è©¦è³‡æ–™çš„åŸå§‹ ID (ç”¨æ–¼è¿½è¹¤ç—…äºº)
            current_test_ids = X_sub.index[test_idx]
            
            X_tr, X_te = X_sub.iloc[train_idx], X_sub.iloc[test_idx]
            X_tr = X_tr.reset_index(drop=True)
            X_te = X_te.reset_index(drop=True)
            y_tr = pd.Series(y_sub[train_idx]) 
            y_te = pd.Series(y_sub[test_idx])
            
            X_tr_p, X_te_p = processor.impute_and_scale(X_tr, X_te, fit=True)
            
            if not X_tr_p.isnull().any().any():
                try:
                    iso = IsolationForest(contamination=0.03, random_state=42, n_jobs=1)
                    outlier_preds = iso.fit_predict(X_tr_p)
                    mask_clean = (outlier_preds == 1) | (y_tr == 1)
                    X_tr_clean = X_tr_p[mask_clean].copy()
                    y_tr_clean = y_tr[mask_clean].copy()
                    removed = len(X_tr_p) - len(X_tr_clean)
                    if removed > 0: print(f"      ğŸ§¹ ç§»é™¤äº† {removed} å€‹ç•°å¸¸æ¨£æœ¬")
                except:
                    X_tr_clean, y_tr_clean = X_tr_p, y_tr
            else:
                X_tr_clean, y_tr_clean = X_tr_p, y_tr

            trainer = ModelTrainer(label, y_tr_clean.sum(), len(y_tr_clean)-y_tr_clean.sum(), base_f1, target_f1, use_stacking)
            trainer.build_models()
            res = trainer.train_and_evaluate(X_tr_clean, X_te_p, y_tr_clean, y_te)
            
            target_shap_model = None
            if 'XGB' in trainer.fitted_models: target_shap_model = trainer.fitted_models['XGB']
            elif 'LGBM' in trainer.fitted_models: target_shap_model = trainer.fitted_models['LGBM']
            
            if target_shap_model:
                try:
                    explainer = shap.TreeExplainer(target_shap_model)
                    shap_vals = explainer.shap_values(X_te_p)
                    if isinstance(shap_vals, list) and len(shap_vals) == 2:
                        shap_values_list.append(shap_vals[1]) 
                    elif isinstance(shap_vals, np.ndarray):
                        if shap_vals.ndim == 2: shap_values_list.append(shap_vals)
                        elif shap_vals.ndim == 3: shap_values_list.append(shap_vals[:, :, 1])
                    X_test_shap_list.append(X_te_p)
                    if hasattr(target_shap_model, 'feature_importances_'):
                        importance_list.append(pd.DataFrame({'Feature': X_te_p.columns, 'Importance': target_shap_model.feature_importances_}))
                except Exception as e: pass

            special = [m for m in res.keys() if m in ['Ensemble', 'Stacking']]
            show_name = max(special, key=lambda k: res[k]['f1_score']) if special else max(res.keys(), key=lambda k: res[k]['f1_score'])
            r = res[show_name]
            
            # [æ–°å¢] æ”¶é›†è©³ç´°é æ¸¬çµæœ (Step 4 é—œéµ)
            # å°‡é€™ä¸€æŠ˜æ‰€æœ‰æ¸¬è©¦ç—…äººçš„é æ¸¬çµæœå­˜å…¥ List
            for i in range(len(test_idx)):
                oof_predictions_list.append({
                    'Subject_ID': current_test_ids[i], # é€™è£¡ä½¿ç”¨çš„æ˜¯åŸå§‹ DataFrame çš„ Index (éœ€ç¢ºä¿ Index å³ ID)
                    'Ground_Truth': y_te.iloc[i],
                    'Pred_Prob': r['y_pred_proba'][i],
                    'Pred_Label': r['y_pred'][i],
                    'Fold': fold_id,
                    'Best_Model': show_name,
                    'Threshold': r['threshold']
                })

            metrics_list.append({
                'F1': r['f1_score'], 'Acc': r['accuracy'], 'AUC': r['auc'],
                'Prec': r['precision'], 'Recall': r['recall'], 
                'Spec': r['specificity'], 'NPV': r['npv']
            })
            
            fpr, tpr, _ = roc_curve(y_te, r['y_pred_proba'])
            tprs.append(np.interp(mean_fpr, fpr, tpr))
            roc_aucs.append(r['auc'])
            
            prec, rec, _ = precision_recall_curve(y_te, r['y_pred_proba'])
            precisions.append(np.interp(mean_recall, rec[::-1], prec[::-1]))
            pr_aucs.append(auc(rec, prec))
            
            y_true_all.extend(y_te)
            y_pred_all.extend(r['y_pred'])
            
            singles = [k for k in res.keys() if k not in ['Ensemble', 'Stacking']]
            if singles:
                best_s_name = max(singles, key=lambda k: res[k]['f1_score'])
                best_s = res[best_s_name]
                is_better = False
                if best_s['f1_score'] > best_model_info['f1']: is_better = True
                elif best_s['f1_score'] == best_model_info['f1'] and best_s['precision'] > best_model_info['p']: is_better = True
                
                if is_better:
                    best_model_info = {
                        "f1": best_s['f1_score'], "p": best_s['precision'],
                        "obj": best_s['model'], "name": best_s_name, "fold": fold_id,
                        "thresh": best_s['threshold'], 
                        "scaler": processor.scaler,
                        "imputer": processor.knn_imputer, 
                        "cols": X_tr_p.columns,
                        "bounds": processor.outlier_bounds_,
                        "r": best_s['recall'], "spec": best_s['specificity'], 
                        "npv": best_s['npv'], "auc": best_s['auc'], "acc": best_s['accuracy']
                    }
            fold_id += 1

        X_sub_p = processor.impute_and_scale(X_sub, fit=True)
        if X_sub_p.isnull().any().any():
            viz.plot_pca_scatter(X_sub_p.fillna(-1), y_sub)
        else:
            viz.plot_pca_scatter(X_sub_p, y_sub)

        df_metrics_fold = pd.DataFrame(metrics_list)
        metrics_summary = df_metrics_fold.mean().to_dict()
        metrics_std = df_metrics_fold.std().to_dict()
        
        df_bar = pd.DataFrame({
            'Metric': list(metrics_summary.keys()),
            'Mean': list(metrics_summary.values()),
            'Std': list(metrics_std.values())
        })
        viz.plot_performance_metrics(df_bar)
        
        for m, v in metrics_summary.items(): 
            overall_metrics_list.append({'Label': label, 'Metric': m, 'Mean': v, 'Std': metrics_std[m]})
        
        viz.plot_roc_curve_with_ci(tprs, mean_fpr, roc_aucs)
        mean_tpr = np.mean(tprs, axis=0); mean_tpr[-1] = 1.0
        overall_roc_data[label] = (mean_fpr, mean_tpr, np.mean(roc_aucs))
        
        viz.plot_pr_curve_with_ci(precisions, mean_recall, pr_aucs, no_skill)
        mean_prec = np.mean(precisions, axis=0)
        overall_pr_data[label] = (mean_recall, mean_prec, np.mean(pr_aucs))
        
        viz.plot_confusion_matrix_aggregated(y_true_all, y_pred_all)
        viz.plot_radar_chart({k: metrics_summary[k] for k in ['F1', 'Acc', 'Prec', 'Recall', 'Spec', 'AUC']})
        
        if importance_list: 
            viz.plot_feature_importance_boxplot(pd.concat(importance_list))
            
        if shap_values_list and X_test_shap_list:
            try:
                if len(shap_values_list) == len(X_test_shap_list):
                    global_shap_values = np.concatenate(shap_values_list, axis=0)
                    global_X_test = pd.concat(X_test_shap_list, axis=0)
                    if global_shap_values.shape[0] == global_X_test.shape[0]:
                        print(f"   ğŸ“Š ç¹ªè£½ Global OOF SHAP Summary (N={global_X_test.shape[0]})...")
                        viz.plot_shap_summary_oof(global_shap_values, global_X_test)
            except Exception as e: 
                print(f"   âš ï¸ SHAP Plot Error: {e}")

        if best_model_info['obj']:
            save_best_model(
                models_dir, label, best_model_info['obj'], 
                best_model_info['scaler'], best_model_info['imputer'],
                best_model_info['cols'], best_model_info['bounds'], 
                best_model_info['thresh']
            )

        # [æ–°å¢] åŒ¯å‡º Step 4 å°ˆç”¨ Excel (å–®ç­†è©³ç´°çµæœ)
        print(f"\nğŸ’¾ æ­£åœ¨åŒ¯å‡º Step 4 åˆ†æç”¨ç¸½è¡¨ (Step1_Predictions_Detail_{label}.xlsx)...")
        if oof_predictions_list:
            df_oof = pd.DataFrame(oof_predictions_list)
            
            # å®šç¾©æƒ³è¦ä¿ç•™çš„åŸå§‹æ¬„ä½ (åŸºæœ¬è³‡æ–™ + å¿ƒç†é‡è¡¨)
            # é€™äº›æ¬„ä½å¦‚æœå­˜åœ¨æ–¼åŸå§‹ Excelï¼Œå°±æœƒè¢«åˆä½µé€²ä¾†
            meta_cols = ['Age', 'Sex', 'BMI']
            # åŠ å…¥å¸¸è¦‹çš„å¿ƒç†é‡è¡¨æ¬„ä½åç¨± (æ ¹æ“šæ‚¨çš„ processors.py æ¨æ¸¬)
            potential_psych_cols = ['phq15', 'haq21', 'cabah', 'bdi', 'bai', 'PHQ_15_Total']
            for c in potential_psych_cols:
                if c in df_full.columns: meta_cols.append(c)
                
            # åˆä½µæ¬„ä½ (ä½¿ç”¨ Subject_ID å°æ‡‰)
            # ç¢ºä¿ meta_cols ç¢ºå¯¦å­˜åœ¨
            cols_to_merge = [c for c in meta_cols if c in df_full.columns]
            
            # å·¦åˆä½µï¼šä»¥é æ¸¬çµæœç‚ºæº–ï¼ŒæŠŠåŸºæœ¬è³‡æ–™è²¼éä¾†
            df_oof = df_oof.merge(df_full[cols_to_merge], left_on='Subject_ID', right_index=True, how='left')
            
            out_path = os.path.join(run_dir, f"Step1_Predictions_Detail_{label}.xlsx")
            df_oof.to_excel(out_path, index=False)
            print(f"âœ… è©³ç´°é æ¸¬è¡¨å·²å„²å­˜: {out_path} (å¯ç›´æ¥ç”¨æ–¼ Step 4)")

        summary_rows.append({
            "Label": label, "BestModel": best_model_info['name'],
            "F1(Best)": best_model_info['f1'], "P(Best)": best_model_info.get('p', 0),
            "R(Best)": best_model_info.get('r', 0), "Spec(Best)": best_model_info.get('spec', 0),
            "NPV(Best)": best_model_info.get('npv', 0), "AUC(Best)": best_model_info.get('auc', 0),
            "ACC(Best)": best_model_info.get('acc', 0),
            "F1(avg)": metrics_summary['F1'], "P(avg)": metrics_summary['Prec'],
            "R(avg)": metrics_summary['Recall'], "Spec(avg)": metrics_summary['Spec'],
            "NPV(avg)": metrics_summary['NPV'], "AUC(avg)": metrics_summary['AUC'],
            "ACC(avg)": metrics_summary['Acc']
        })

    print("\nğŸ“Š æ­£åœ¨ç¹ªè£½å¤šç–¾ç—…æ¯”è¼ƒç¸½åœ–...")
    viz_summary = Visualizer("Comparison", run_dir, sub_folder="Summary_Comparison")
    if overall_metrics_list: viz_summary.plot_multilabel_metrics(pd.DataFrame(overall_metrics_list))
    if overall_roc_data: viz_summary.plot_multilabel_roc(overall_roc_data)
    if overall_pr_data: viz_summary.plot_multilabel_pr(overall_pr_data)

    if summary_rows:
        res_df = pd.DataFrame(summary_rows)
        res_df.to_excel(os.path.join(run_dir, "Results_Summary.xlsx"), index=False)
        cols = ["Label", "BestModel", 
                "F1(Best)", "P(Best)", "R(Best)", "Spec(Best)", "NPV(Best)", "AUC(Best)", "ACC(Best)", 
                "F1(avg)", "P(avg)", "R(avg)", "Spec(avg)", "NPV(avg)", "AUC(avg)", "ACC(avg)"]
        pretty_print_table(res_df[cols], title="æœ€çµ‚çµæœæ‘˜è¦")
        print(f"\næ‰€æœ‰çµæœèˆ‡æ¯”è¼ƒåœ–è¡¨å·²å­˜è‡³: {run_dir}/plots")


def run_external_validation(models_dir_input, file_path, sheet_name, processor_cls=ProcessorBaseline4):
    """
    å¤–éƒ¨é©—è­‰å‡½å¼ (å« GAD è‡ªé©æ‡‰é–¾å€¼æŒ‘é¸)
    """
    print("\n" + "="*70)
    print(f"åŸ·è¡Œå¤–éƒ¨é©—è­‰ (Data1)")
    print(f"æ¨¡å‹ä¾†æº: {models_dir_input}")
    print(f"ä½¿ç”¨è™•ç†å™¨: {processor_cls.__name__}")
    print("="*70)
    
    if not os.path.exists(models_dir_input):
        print(f"æ‰¾ä¸åˆ°æ¨¡å‹è³‡æ–™å¤¾: {models_dir_input}")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    val_out_dir = os.path.join(os.path.dirname(models_dir_input), f"Validation_Data1_{timestamp}")
    os.makedirs(val_out_dir, exist_ok=True)

    processor = processor_cls(file_path, sheet_name)
    if not processor.load_data(): return
    if not processor.prepare_features_and_labels(): return
    
    df_full = processor.df
    X_full = processor.X
    label_names = ['SSD', 'MDD', 'Panic', 'GAD']
    
    results = []
    
    for label in label_names:
        print(f"\nğŸ” é©—è­‰ï¼š{label}")
        
        info = load_best_model_and_meta(models_dir_input, label)
        if not info:
            print(f"  ç„¡æ³•è¼‰å…¥ {label} æ¨¡å‹ï¼Œè·³é")
            continue
            
        if label not in df_full.columns or 'Health' not in df_full.columns: continue
        
        mask_valid = (
            df_full['Health'].isin([0, 1]) &
            df_full[label].isin([0, 1]) &
            ((df_full['Health'] == 1) | (df_full[label] == 1))
        )
        df_sub = df_full.loc[mask_valid].copy()
        X_sub = X_full.loc[mask_valid].copy()
        
        mask_xor = (df_sub['Health'] == 1) ^ (df_sub[label] == 1)
        df_sub = df_sub.loc[mask_xor]
        X_sub = X_sub.loc[mask_xor]
        
        y_sub = np.where(df_sub[label] == 1, 1, 0)
        
        if len(y_sub) == 0:
            print("  ç„¡æœ‰æ•ˆæ¨£æœ¬")
            continue
        
        print(f"   æ¨£æœ¬æ•¸: {len(y_sub)} (æ­£ä¾‹={y_sub.sum()}, è² ä¾‹={len(y_sub)-y_sub.sum()})")

        required_cols = info['feature_columns']
        X_eval = pd.DataFrame(index=X_sub.index)
        
        missing_cols = []
        for col in required_cols:
            if col in X_sub.columns:
                X_eval[col] = X_sub[col]
            else:
                X_eval[col] = np.nan
                missing_cols.append(col)
        
        if missing_cols:
            print(f"   âš ï¸ è­¦å‘Šï¼šé©—è­‰é›†ç¼ºå°‘ä»¥ä¸‹è¨“ç·´ç‰¹å¾µ (å°‡è£œ NaN): {missing_cols[:5]}...")
        
        X_eval = X_eval[required_cols]
            
        processor.outlier_bounds_ = info['outlier_bounds']
        processor.knn_imputer = info['imputer']
        processor.scaler = info['scaler']
        
        X_eval_p = processor.impute_and_scale(X_eval, fit=False)
        
        model = info['model']
        try:
            proba = model.predict_proba(X_eval_p)[:, 1]
        except:
            proba = model.predict(X_eval_p)
        
        try: auc_val = roc_auc_score(y_sub, proba)
        except: auc_val = np.nan

        original_threshold = info['threshold']
        
        # å»ºç«‹å€™é¸é–¾å€¼åˆ—è¡¨ï¼šåŒ…å«åŸæœ¬çš„ï¼Œä»¥åŠ 0.3~0.7
        test_candidates = [original_threshold] + list(np.arange(0.3, 0.75, 0.05))
        test_thresholds = sorted(list(set(test_candidates)))
        
        print(f"   ğŸ“Š é–¾å€¼è¨ºæ–·èˆ‡è‡ªå‹•æŒ‘é¸ (GADç›®æ¨™: F1>0.4, Rec>0.4, Spec>0.6)...")
        
        best_compliant_stats = None
        best_f1_compliant = -1.0
        
        original_stats = None

        for th in test_thresholds:
            pred_th = (proba >= th).astype(int)
            
            f1_th = f1_score(y_sub, pred_th)
            acc_th = accuracy_score(y_sub, pred_th)
            prec_th = precision_score(y_sub, pred_th, zero_division=0)
            rec_th = recall_score(y_sub, pred_th, zero_division=0)
            
            cm_th = confusion_matrix(y_sub, pred_th, labels=[0, 1])
            tn, fp, fn, tp = cm_th.ravel() if cm_th.size==4 else (0,0,0,0)
            spec_th = tn/(tn+fp) if (tn+fp)>0 else 0
            npv_th = tn/(tn+fn) if (tn+fn)>0 else 0
            
            is_original = abs(th - original_threshold) < 1e-9
            
            # å»ºç«‹ Stats ç‰©ä»¶
            stats = {
                "pred": pred_th, "f1": f1_th, "acc": acc_th, 
                "prec": prec_th, "rec": rec_th, "spec": spec_th, "npv": npv_th,
                "threshold": th, "is_original": is_original
            }
            
            if is_original:
                original_stats = stats
                
            # [æŒ‘é¸é‚è¼¯] æª¢æŸ¥æ˜¯å¦ç¬¦åˆ GAD æ¢ä»¶
            is_compliant = True
            if label == 'GAD':
                if not (f1_th > 0.4 and rec_th > 0.4 and spec_th > 0.6):
                    is_compliant = False
            
            marker = "  "
            if is_original: marker = "â­"
            if is_compliant and label == 'GAD' and not is_original: marker = "âœ¨"
            
            # æ›´æ–°æœ€ä½³ç¬¦åˆæ¢ä»¶è€… (Maximize F1)
            if is_compliant:
                if f1_th > best_f1_compliant:
                    best_f1_compliant = f1_th
                    best_compliant_stats = stats

            # ç°¡åŒ– Log è¼¸å‡º
            if is_original or (is_compliant and label=='GAD') or abs(th-0.5)<0.01:
                print(f"      {marker} Th={th:.2f} | F1={f1_th:.4f} | Rec={rec_th:.4f} | Spec={spec_th:.4f}")

        # æ±ºå®šæœ€çµ‚è¼¸å‡ºçš„çµæœ
        # å¦‚æœæœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„é–¾å€¼ï¼Œå°±ç”¨å®ƒï¼›å¦å‰‡å›é€€åˆ° Original
        if label == 'GAD' and best_compliant_stats is not None:
            final_stats = best_compliant_stats
            if not final_stats['is_original']:
                print(f"      ğŸ‘‰ [Selected] æ¡ç”¨é©æ‡‰æ€§é–¾å€¼ Th={final_stats['threshold']:.2f} (ç¬¦åˆ GAD æ¢ä»¶)")
            else:
                print(f"      ğŸ‘‰ [Selected] æ¡ç”¨åŸå§‹é–¾å€¼ (å·²ç¬¦åˆæ¢ä»¶)")
        else:
            final_stats = original_stats
            # é˜²å‘†ï¼šå¦‚æœ original_stats æ²’æŠ“åˆ° (æµ®é»æ•¸èª¤å·®)ï¼Œé‡ç®—ä¸€æ¬¡
            if final_stats is None:
                pred = (proba >= original_threshold).astype(int)
                cm = confusion_matrix(y_sub, pred, labels=[0, 1])
                tn, fp, fn, tp = cm.ravel()
                final_stats = {
                    "pred": pred,
                    "f1": f1_score(y_sub, pred),
                    "acc": accuracy_score(y_sub, pred),
                    "prec": precision_score(y_sub, pred, zero_division=0),
                    "rec": recall_score(y_sub, pred, zero_division=0),
                    "spec": tn/(tn+fp) if (tn+fp)>0 else 0,
                    "npv": tn/(tn+fn) if (tn+fn)>0 else 0,
                    "threshold": original_threshold
                }
                print(f"      ğŸ‘‰ æ¡ç”¨åŸå§‹é–¾å€¼ (Fallback)")

        results.append({
            "Label": label, 
            "F1": final_stats['f1'], "Acc": final_stats['acc'], "AUC": auc_val, 
            "Spec": final_stats['spec'], "NPV": final_stats['npv'], 
            "Prec": final_stats['prec'], "Recall": final_stats['rec'],
            "Threshold": final_stats['threshold']
        })
        
        viz = Visualizer(label, val_out_dir, sub_folder=label)
        viz.plot_confusion_matrix_aggregated(y_sub, final_stats['pred'])
        if len(np.unique(y_sub)) > 1:
            fpr, tpr, _ = roc_curve(y_sub, proba)
            viz.plot_roc_curve_with_ci([tpr], fpr, [auc_val])
            
    if results:
        res_df = pd.DataFrame(results)
        excel_path = os.path.join(val_out_dir, "External_Validation_Results.xlsx")
        res_df.to_excel(excel_path, index=False)
        cols = ["Label", "F1", "Prec", "Recall", "Spec", "NPV", "AUC", "Acc", "Threshold"]
        pretty_print_table(res_df[cols], title="å¤–éƒ¨é©—è­‰çµæœæ‘˜è¦ (Selected)")
        print(f"\nå¤–éƒ¨é©—è­‰å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³: {val_out_dir}")
    else:
        print("\nç„¡æ³•ç”¢ç”Ÿä»»ä½•é©—è­‰çµæœ (å¯èƒ½ç¼ºè³‡æ–™æˆ–æ¨¡å‹)")