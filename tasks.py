import os
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    roc_curve, precision_recall_curve, auc, 
    f1_score, accuracy_score, precision_score, 
    recall_score, confusion_matrix, roc_auc_score
)
from sklearn.ensemble import IsolationForest

# å¼•ç”¨è‡ªå®šç¾©æ¨¡çµ„
from model_trainer import ModelTrainer
from utils import save_best_model, pretty_print_table, load_best_model_and_meta
from visualization import Visualizer
import shap 

# å¼•ç”¨ Processors
from processors import (
    ProcessorHRV, 
    ProcessorPsych, 
    ProcessorBaselineAll, 
    ProcessorFullV62, 
    DataProcessorBaseline
)

def run_binary_task(task_name, file_path, sheet_name, processor_cls, use_stacking=True):
    print("\n" + "="*70)
    print(f"åŸ·è¡Œä»»å‹™: {task_name} (AutoML & SHAP-OOF Version)")
    print("="*70)
    
    timestamp = datetime.now().strftime(f"{task_name}_%Y%m%d_%H%M%S")
    run_dir = os.path.join(os.getcwd(), "runs1", timestamp)
    models_dir = os.path.join(run_dir, "models")
    os.makedirs(models_dir, exist_ok=True)
    
    # 1. è¼‰å…¥ä¸¦æº–å‚™æ•¸æ“š
    processor = processor_cls(file_path, sheet_name)
    if not processor.load_data(): return
    if not processor.prepare_features_and_labels(): return
    
    label_names = ['SSD', 'MDD', 'Panic', 'GAD']
    y_dict = processor.y_dict
    df_full = processor.df
    X_full = processor.X
    
    summary_rows = []
    
    # å…¨åŸŸæ¯”è¼ƒå®¹å™¨
    overall_roc_data = {}
    overall_pr_data = {}
    overall_metrics_list = [] 
    
    for label in label_names:
        if label not in y_dict: continue
        print(f"\nğŸ©º è¨ºæ–·ï¼š{label} vs Health")
        
        # 2. å®šç¾©æ­£è² æ¨£æœ¬ (Disease vs Health Only)
        mask_disease = df_full[label] == 1
        mask_health = (df_full['Health'] == 1) & (df_full[label] == 0)
        mask_valid = mask_disease | mask_health
        X_sub = X_full.loc[mask_valid].copy()
        y_sub = np.where(mask_disease[mask_valid], 1, 0)
        
        # åˆå§‹åŒ–è¦–è¦ºåŒ–ç‰©ä»¶
        viz = Visualizer(label, run_dir, sub_folder=label)

        # [NEW] ç¹ªè£½ Correlation Matrix (åœ¨ CV ä¹‹å‰)
        # ç‚ºäº†ç•«åœ–ï¼Œæˆ‘å€‘å…ˆåšä¸€æ¬¡æ•´é«”çš„ Impute (fit=True)
        # æ³¨æ„ï¼šé€™åªæ˜¯ç‚ºäº†ç•« EDA åœ–ï¼Œä¸æœƒç”¨æ–¼å¾ŒçºŒè¨“ç·´ (è¨“ç·´æœƒé‡æ–°åœ¨ Fold å…§è™•ç†)
        print("   ğŸ“Š ç¹ªè£½ç‰¹å¾µç›¸é—œæ€§çŸ©é™£ (EDA)...")
        try:
            X_sub_corr_p = processor.impute_and_scale(X_sub, fit=True)
            viz.plot_correlation_matrix(X_sub_corr_p)
        except Exception as e:
            print(f"   âš ï¸ ç„¡æ³•ç¹ªè£½ Correlation Matrix: {e}")

        # è¨­å®šç›®æ¨™ F1 (åƒ…ä¾› log é¡¯ç¤ºåƒè€ƒ)
        base_f1 = {'SSD':0.66, 'MDD':0.46, 'Panic':0.50, 'GAD':0.57}.get(label, 0.5)
        target_f1 = {'SSD':0.75, 'MDD':0.75, 'Panic':0.55, 'GAD':0.70}.get(label, 0.7)
        
        # 3. å¤–éƒ¨è¿´åœˆ (Outer CV Loop)
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        metrics_list = []
        tprs = []; mean_fpr = np.linspace(0, 1, 100)
        roc_aucs = []
        precisions = []; mean_recall = np.linspace(0, 1, 100)
        pr_aucs = []
        no_skill = y_sub.sum() / len(y_sub)
        
        y_true_all = []; y_pred_all = []
        
        # [NEW] SHAP æ”¶é›†å®¹å™¨ (ç”¨æ–¼ OOF ä¸²æ¥)
        shap_values_list = []
        X_test_shap_list = []
        importance_list = []
        
        best_model_info = {"f1": -1.0, "p": -1.0, "obj": None, "name": None}
        
        fold_id = 1
        for train_idx, test_idx in skf.split(X_sub, y_sub):
            print(f"\n   ğŸ“‚ Fold {fold_id}/5")
            X_tr, X_te = X_sub.iloc[train_idx], X_sub.iloc[test_idx]
            
            X_tr = X_tr.reset_index(drop=True)
            X_te = X_te.reset_index(drop=True)
            y_tr = pd.Series(y_sub[train_idx]) 
            y_te = pd.Series(y_sub[test_idx])
            
            # 4. æ•¸æ“šå‰è™•ç† (Inside Fold -> No Leakage)
            X_tr_p, X_te_p = processor.impute_and_scale(X_tr, X_te, fit=True)
            
            # 5. Isolation Forest
            iso = IsolationForest(contamination=0.03, random_state=42, n_jobs=1)
            outlier_preds = iso.fit_predict(X_tr_p)
            mask_clean = (outlier_preds == 1) | (y_tr == 1)
            X_tr_clean = X_tr_p[mask_clean].copy()
            y_tr_clean = y_tr[mask_clean].copy()
            
            removed = len(X_tr_p) - len(X_tr_clean)
            if removed > 0: print(f"      ğŸ§¹ ç§»é™¤äº† {removed} å€‹ç•°å¸¸æ¨£æœ¬")

            # 6. è¨“ç·´ (AutoML)
            trainer = ModelTrainer(label, y_tr_clean.sum(), len(y_tr_clean)-y_tr_clean.sum(), base_f1, target_f1, use_stacking)
            trainer.build_models()
            res = trainer.train_and_evaluate(X_tr_clean, X_te_p, y_tr_clean, y_te)
            
            # 7. SHAP æ”¶é›†
            # å„ªå…ˆå¾æ¨¹æ¨¡å‹ (XGB/LGBM) æå–è§£é‡‹
            target_shap_model = None
            if 'XGB' in trainer.fitted_models: target_shap_model = trainer.fitted_models['XGB']
            elif 'LGBM' in trainer.fitted_models: target_shap_model = trainer.fitted_models['LGBM']
            
            if target_shap_model:
                try:
                    explainer = shap.TreeExplainer(target_shap_model)
                    shap_vals = explainer.shap_values(X_te_p)
                    
                    # è™•ç†ä¸åŒå¥—ä»¶å›å‚³æ ¼å¼å·®ç•° (List vs Array)
                    if isinstance(shap_vals, list) and len(shap_vals) == 2:
                        # Binary Case:å– class 1
                        shap_values_list.append(shap_vals[1]) 
                    elif isinstance(shap_vals, np.ndarray):
                        # å¯èƒ½æ˜¯ (samples, features) æˆ– (samples, features, classes)
                        if shap_vals.ndim == 2:
                            shap_values_list.append(shap_vals)
                        elif shap_vals.ndim == 3: # LGBM æœ‰æ™‚æœƒé€™æ¨£
                            shap_values_list.append(shap_vals[:, :, 1])
                    
                    X_test_shap_list.append(X_te_p)
                    
                    if hasattr(target_shap_model, 'feature_importances_'):
                        importance_list.append(pd.DataFrame({
                            'Feature': X_te_p.columns, 'Importance': target_shap_model.feature_importances_
                        }))
                except Exception as e:
                    # ä¸ä¸­æ–·æµç¨‹
                    pass

            # 8. é¸æ“‡ç•¶æŠ˜æœ€ä½³æ¨¡å‹ (ç”¨æ–¼ Metrics)
            special = [m for m in res.keys() if m in ['Ensemble', 'Stacking']]
            show_name = max(special, key=lambda k: res[k]['f1_score']) if special else max(res.keys(), key=lambda k: res[k]['f1_score'])
            r = res[show_name]
            
            metrics_list.append({
                'F1': r['f1_score'], 'Acc': r['accuracy'], 'AUC': r['auc'],
                'Prec': r['precision'], 'Recall': r['recall'], 
                'Spec': r['specificity'], 'NPV': r['npv']
            })
            
            fpr, tpr, _ = roc_curve(y_te, r['y_pred_proba'])
            tprs.append(np.interp(mean_fpr, fpr, tpr))
            roc_aucs.append(r['auc'])
            
            prec, rec, _ = precision_recall_curve(y_te, r['y_pred_proba'])
            precisions.append(np.interp(mean_recall, rec[::-1], prec[::-1]))
            pr_aucs.append(auc(rec, prec))
            
            y_true_all.extend(y_te)
            y_pred_all.extend(r['y_pred'])
            
            # 9. æ›´æ–°æ•´é«”æœ€ä½³æ¨¡å‹ (åƒ…ä¿å­˜ Single Model)
            singles = [k for k in res.keys() if k not in ['Ensemble', 'Stacking']]
            if singles:
                best_s_name = max(singles, key=lambda k: res[k]['f1_score'])
                best_s = res[best_s_name]
                is_better = False
                if best_s['f1_score'] > best_model_info['f1']: is_better = True
                elif best_s['f1_score'] == best_model_info['f1'] and best_s['precision'] > best_model_info['p']: is_better = True
                
                if is_better:
                    best_model_info = {
                        "f1": best_s['f1_score'], "p": best_s['precision'],
                        "obj": best_s['model'], "name": best_s_name, "fold": fold_id,
                        "thresh": best_s['threshold'], 
                        "scaler": processor.scaler,
                        "imputer": processor.knn_imputer, 
                        "cols": X_tr_p.columns,
                        "bounds": processor.outlier_bounds_,
                        "r": best_s['recall'], "spec": best_s['specificity'], 
                        "npv": best_s['npv'], "auc": best_s['auc'], "acc": best_s['accuracy']
                    }
            fold_id += 1

        # 10. è¦–è¦ºåŒ– (Loop çµæŸå¾Œ)
        X_sub_p = processor.impute_and_scale(X_sub, fit=True)
        viz.plot_pca_scatter(X_sub_p, y_sub)

        df_metrics_fold = pd.DataFrame(metrics_list)
        metrics_summary = df_metrics_fold.mean().to_dict()
        metrics_std = df_metrics_fold.std().to_dict()
        
        df_bar = pd.DataFrame({
            'Metric': list(metrics_summary.keys()),
            'Mean': list(metrics_summary.values()),
            'Std': list(metrics_std.values())
        })
        viz.plot_performance_metrics(df_bar)
        
        for m, v in metrics_summary.items(): 
            overall_metrics_list.append({'Label': label, 'Metric': m, 'Mean': v, 'Std': metrics_std[m]})
        
        viz.plot_roc_curve_with_ci(tprs, mean_fpr, roc_aucs)
        mean_tpr = np.mean(tprs, axis=0); mean_tpr[-1] = 1.0
        overall_roc_data[label] = (mean_fpr, mean_tpr, np.mean(roc_aucs))
        
        viz.plot_pr_curve_with_ci(precisions, mean_recall, pr_aucs, no_skill)
        mean_prec = np.mean(precisions, axis=0)
        overall_pr_data[label] = (mean_recall, mean_prec, np.mean(pr_aucs))
        
        viz.plot_confusion_matrix_aggregated(y_true_all, y_pred_all)
        viz.plot_radar_chart({k: metrics_summary[k] for k in ['F1', 'Acc', 'Prec', 'Recall', 'Spec', 'AUC']})
        
        if importance_list: 
            viz.plot_feature_importance_boxplot(pd.concat(importance_list))
            
        # [NEW] ç¹ªè£½ Global OOF SHAP
        # ä¸²æ¥ 5 å€‹ Fold çš„ SHAP èˆ‡ Feature Data
        if shap_values_list and X_test_shap_list:
            try:
                # ç°¡å–®æª¢æŸ¥é•·åº¦æ˜¯å¦ä¸€è‡´
                if len(shap_values_list) == len(X_test_shap_list):
                    global_shap_values = np.concatenate(shap_values_list, axis=0)
                    global_X_test = pd.concat(X_test_shap_list, axis=0)
                    
                    # ç¢ºä¿æ²’æœ‰å½¢ç‹€ä¸åŒ¹é… (ä¾‹å¦‚å…¶ä¸­ä¸€å€‹ fold æ˜¯ç©ºçš„)
                    if global_shap_values.shape[0] == global_X_test.shape[0]:
                        print(f"   ğŸ“Š ç¹ªè£½ Global OOF SHAP Summary (N={global_X_test.shape[0]})...")
                        viz.plot_shap_summary_oof(global_shap_values, global_X_test)
                else:
                    print("   âš ï¸ SHAP åˆ—è¡¨é•·åº¦ä¸ä¸€è‡´ï¼Œè·³éç¹ªåœ–")
            except Exception as e: 
                print(f"   âš ï¸ SHAP Plot Error: {e}")

        # ä¿å­˜æ¨¡å‹
        if best_model_info['obj']:
            save_best_model(
                models_dir, label, best_model_info['obj'], 
                best_model_info['scaler'], best_model_info['imputer'],
                best_model_info['cols'], best_model_info['bounds'], 
                best_model_info['thresh']
            )

        summary_rows.append({
            "Label": label, "BestModel": best_model_info['name'],
            "F1(Best)": best_model_info['f1'], "P(Best)": best_model_info.get('p', 0),
            "R(Best)": best_model_info.get('r', 0), "Spec(Best)": best_model_info.get('spec', 0),
            "NPV(Best)": best_model_info.get('npv', 0), "AUC(Best)": best_model_info.get('auc', 0),
            "ACC(Best)": best_model_info.get('acc', 0),
            "F1(avg)": metrics_summary['F1'], "P(avg)": metrics_summary['Prec'],
            "R(avg)": metrics_summary['Recall'], "Spec(avg)": metrics_summary['Spec'],
            "NPV(avg)": metrics_summary['NPV'], "AUC(avg)": metrics_summary['AUC'],
            "ACC(avg)": metrics_summary['Acc']
        })

    print("\nğŸ“Š æ­£åœ¨ç¹ªè£½å¤šç–¾ç—…æ¯”è¼ƒç¸½åœ–...")
    viz_summary = Visualizer("Comparison", run_dir, sub_folder="Summary_Comparison")
    if overall_metrics_list: viz_summary.plot_multilabel_metrics(pd.DataFrame(overall_metrics_list))
    if overall_roc_data: viz_summary.plot_multilabel_roc(overall_roc_data)
    if overall_pr_data: viz_summary.plot_multilabel_pr(overall_pr_data)

    if summary_rows:
        res_df = pd.DataFrame(summary_rows)
        res_df.to_excel(os.path.join(run_dir, "Results_Summary.xlsx"), index=False)
        cols = ["Label", "BestModel", 
                "F1(Best)", "P(Best)", "R(Best)", "Spec(Best)", "NPV(Best)", "AUC(Best)", "ACC(Best)", 
                "F1(avg)", "P(avg)", "R(avg)", "Spec(avg)", "NPV(avg)", "AUC(avg)", "ACC(avg)"]
        pretty_print_table(res_df[cols], title="æœ€çµ‚çµæœæ‘˜è¦")
        print(f"\næ‰€æœ‰çµæœèˆ‡æ¯”è¼ƒåœ–è¡¨å·²å­˜è‡³: {run_dir}/plots")


# def run_external_validation(models_dir_input, file_path, sheet_name, processor_cls=DataProcessorBaseline):
#     """
#     å¤–éƒ¨é©—è­‰å‡½å¼
#     æ›´æ–°ï¼šå¢åŠ  processor_cls åƒæ•¸ï¼Œå…è¨±å‚³å…¥ FullV62 ç­‰é«˜éšè™•ç†å™¨
#     """
#     print("\n" + "="*70)
#     print(f"åŸ·è¡Œå¤–éƒ¨é©—è­‰ (Data1)")
#     print(f"æ¨¡å‹ä¾†æº: {models_dir_input}")
#     print(f"ä½¿ç”¨è™•ç†å™¨: {processor_cls.__name__}")
#     print("="*70)
    
#     if not os.path.exists(models_dir_input):
#         print(f"æ‰¾ä¸åˆ°æ¨¡å‹è³‡æ–™å¤¾: {models_dir_input}")
#         return

#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     val_out_dir = os.path.join(os.path.dirname(models_dir_input), f"Validation_Data1_{timestamp}")
#     os.makedirs(val_out_dir, exist_ok=True)

#     # è¼‰å…¥æ•¸æ“š
#     processor = processor_cls(file_path, sheet_name)
    
#     if not processor.load_data(): return
#     if not processor.prepare_features_and_labels(): return
    
#     df_full = processor.df
#     X_full = processor.X
#     label_names = ['SSD', 'MDD', 'Panic', 'GAD']
    
#     results = []
    
#     for label in label_names:
#         print(f"\nğŸ” é©—è­‰ï¼š{label}")
        
#         # è¼‰å…¥æ¨¡å‹ Metadata
#         info = load_best_model_and_meta(models_dir_input, label)
#         if not info:
#             print(f"  ç„¡æ³•è¼‰å…¥ {label} æ¨¡å‹ï¼Œè·³é")
#             continue
            
#         # ç¯©é¸è³‡æ–™ (Data1 çš„çµæ§‹)
#         if label not in df_full.columns or 'Health' not in df_full.columns: continue
        
#         mask_valid = (
#             df_full['Health'].isin([0, 1]) &
#             df_full[label].isin([0, 1]) &
#             ((df_full['Health'] == 1) | (df_full[label] == 1))
#         )
#         df_sub = df_full.loc[mask_valid].copy()
#         X_sub = X_full.loc[mask_valid].copy()
        
#         mask_xor = (df_sub['Health'] == 1) ^ (df_sub[label] == 1)
#         df_sub = df_sub.loc[mask_xor]
#         X_sub = X_sub.loc[mask_xor]
        
#         y_sub = np.where(df_sub[label] == 1, 1, 0)
        
#         if len(y_sub) == 0:
#             print("  ç„¡æœ‰æ•ˆæ¨£æœ¬")
#             continue
        
#         print(f"   æ¨£æœ¬æ•¸: {len(y_sub)} (æ­£ä¾‹={y_sub.sum()}, è² ä¾‹={len(y_sub)-y_sub.sum()})")

#         # [Check] å°é½Šç‰¹å¾µï¼šéå¸¸é‡è¦ï¼ç¢ºä¿é©—è­‰é›†çš„ç‰¹å¾µèˆ‡è¨“ç·´é›†å®Œå…¨ä¸€è‡´
#         required_cols = info['feature_columns']
#         X_eval = pd.DataFrame(index=X_sub.index)
        
#         missing_cols = []
#         for col in required_cols:
#             if col in X_sub.columns:
#                 X_eval[col] = X_sub[col]
#             else:
#                 X_eval[col] = np.nan
#                 missing_cols.append(col)
        
#         if missing_cols:
#             print(f"   âš ï¸ è­¦å‘Šï¼šé©—è­‰é›†ç¼ºå°‘ä»¥ä¸‹è¨“ç·´ç‰¹å¾µ (å°‡è£œ NaN): {missing_cols[:5]}...")
        
#         X_eval = X_eval[required_cols]
            
#         # å¥—ç”¨è¨“ç·´æ™‚çš„ Preprocessor ç‹€æ…‹ (Scaler, Imputer, Bounds)
#         processor.outlier_bounds_ = info['outlier_bounds']
#         processor.knn_imputer = info['imputer']
#         processor.scaler = info['scaler']
        
#         # ä½¿ç”¨ fit=Falseï¼Œç¢ºä¿å®Œå…¨ä¾ç…§è¨“ç·´é›†çš„åƒæ•¸è½‰æ›
#         X_eval_p = processor.impute_and_scale(X_eval, fit=False)
        
#         # æ¨è«–
#         model = info['model']
#         try:
#             proba = model.predict_proba(X_eval_p)[:, 1]
#         except:
#             proba = model.predict(X_eval_p)
        
#         # ä½¿ç”¨æ¨¡å‹è¨“ç·´æ™‚å­˜ä¸‹ä¾†çš„æœ€ä½³é–¾å€¼ (é€™å€‹é–¾å€¼å·²ç¶“æ˜¯ç¶“é CV é©—è­‰çš„)
#         threshold = info['threshold']
#         pred = (proba >= threshold).astype(int)
        
#         f1 = f1_score(y_sub, pred)
#         acc = accuracy_score(y_sub, pred)
#         prec = precision_score(y_sub, pred, zero_division=0)
#         rec = recall_score(y_sub, pred, zero_division=0)
        
#         cm = confusion_matrix(y_sub, pred, labels=[0, 1])
#         tn, fp, fn, tp = cm.ravel() if cm.size==4 else (0,0,0,0)
#         spec = tn/(tn+fp) if (tn+fp)>0 else 0
#         npv = tn/(tn+fn) if (tn+fn)>0 else 0
        
#         try: auc_val = roc_auc_score(y_sub, proba)
#         except: auc_val = np.nan
        
#         print(f"   â†’ Result: F1={f1:.4f}, Acc={acc:.4f}, AUC={auc_val:.4f}, Spec={spec:.4f} (Th={threshold:.2f})")
        
#         results.append({
#             "Label": label, "F1": f1, "Acc": acc, "AUC": auc_val, 
#             "Spec": spec, "NPV": npv, "Prec": prec, "Recall": rec,
#             "Threshold": threshold
#         })
        
#         # é©—è­‰é›†ç¹ªåœ–
#         viz = Visualizer(label, val_out_dir, sub_folder=label)
#         viz.plot_confusion_matrix_aggregated(y_sub, pred)
#         if len(np.unique(y_sub)) > 1:
#             fpr, tpr, _ = roc_curve(y_sub, proba)
#             viz.plot_roc_curve_with_ci([tpr], fpr, [auc_val])
            
#     if results:
#         res_df = pd.DataFrame(results)
#         excel_path = os.path.join(val_out_dir, "External_Validation_Results.xlsx")
#         res_df.to_excel(excel_path, index=False)
        
#         cols = ["Label", "F1", "Prec", "Recall", "Spec", "NPV", "AUC", "Acc", "Threshold"]
        
#         pretty_print_table(res_df[cols], title="å¤–éƒ¨é©—è­‰çµæœæ‘˜è¦")
#         print(f"\nå¤–éƒ¨é©—è­‰å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³: {val_out_dir}")
#     else:
#         print("\nç„¡æ³•ç”¢ç”Ÿä»»ä½•é©—è­‰çµæœ (å¯èƒ½ç¼ºè³‡æ–™æˆ–æ¨¡å‹)")

def run_external_validation(models_dir_input, file_path, sheet_name, processor_cls=DataProcessorBaseline):
    """
    å¤–éƒ¨é©—è­‰å‡½å¼ (å«é–¾å€¼è¨ºæ–·åŠŸèƒ½)
    """
    print("\n" + "="*70)
    print(f"åŸ·è¡Œå¤–éƒ¨é©—è­‰ (Data1)")
    print(f"æ¨¡å‹ä¾†æº: {models_dir_input}")
    print(f"ä½¿ç”¨è™•ç†å™¨: {processor_cls.__name__}")
    print("="*70)
    
    if not os.path.exists(models_dir_input):
        print(f"æ‰¾ä¸åˆ°æ¨¡å‹è³‡æ–™å¤¾: {models_dir_input}")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    val_out_dir = os.path.join(os.path.dirname(models_dir_input), f"Validation_Data1_{timestamp}")
    os.makedirs(val_out_dir, exist_ok=True)

    # 1. è¼‰å…¥æ•¸æ“š
    processor = processor_cls(file_path, sheet_name)
    
    if not processor.load_data(): return
    if not processor.prepare_features_and_labels(): return
    
    df_full = processor.df
    X_full = processor.X
    label_names = ['SSD', 'MDD', 'Panic', 'GAD']
    
    results = []
    
    for label in label_names:
        print(f"\nğŸ” é©—è­‰ï¼š{label}")
        
        # 2. è¼‰å…¥æ¨¡å‹ Metadata
        info = load_best_model_and_meta(models_dir_input, label)
        if not info:
            print(f"  ç„¡æ³•è¼‰å…¥ {label} æ¨¡å‹ï¼Œè·³é")
            continue
            
        # 3. ç¯©é¸è³‡æ–™ (Data1 çš„çµæ§‹)
        if label not in df_full.columns or 'Health' not in df_full.columns: continue
        
        mask_valid = (
            df_full['Health'].isin([0, 1]) &
            df_full[label].isin([0, 1]) &
            ((df_full['Health'] == 1) | (df_full[label] == 1))
        )
        df_sub = df_full.loc[mask_valid].copy()
        X_sub = X_full.loc[mask_valid].copy()
        
        # ç¢ºä¿äº’æ–¥ (Health=1 xor Disease=1)
        mask_xor = (df_sub['Health'] == 1) ^ (df_sub[label] == 1)
        df_sub = df_sub.loc[mask_xor]
        X_sub = X_sub.loc[mask_xor]
        
        y_sub = np.where(df_sub[label] == 1, 1, 0)
        
        if len(y_sub) == 0:
            print("  ç„¡æœ‰æ•ˆæ¨£æœ¬")
            continue
        
        print(f"   æ¨£æœ¬æ•¸: {len(y_sub)} (æ­£ä¾‹={y_sub.sum()}, è² ä¾‹={len(y_sub)-y_sub.sum()})")

        # 4. å°é½Šç‰¹å¾µ (Feature Alignment)
        required_cols = info['feature_columns']
        X_eval = pd.DataFrame(index=X_sub.index)
        
        missing_cols = []
        for col in required_cols:
            if col in X_sub.columns:
                X_eval[col] = X_sub[col]
            else:
                X_eval[col] = np.nan
                missing_cols.append(col)
        
        if missing_cols:
            print(f"   âš ï¸ è­¦å‘Šï¼šé©—è­‰é›†ç¼ºå°‘ä»¥ä¸‹è¨“ç·´ç‰¹å¾µ (å°‡è£œ NaN): {missing_cols[:5]}...")
        
        X_eval = X_eval[required_cols]
            
        # 5. å¥—ç”¨è¨“ç·´æ™‚çš„ Preprocessor (Scaler, Imputer, Bounds)
        processor.outlier_bounds_ = info['outlier_bounds']
        processor.knn_imputer = info['imputer']
        processor.scaler = info['scaler']
        
        # fit=False ç¢ºä¿ä½¿ç”¨è¨“ç·´é›†çš„çµ±è¨ˆåƒæ•¸
        X_eval_p = processor.impute_and_scale(X_eval, fit=False)
        
        # 6. æ¨è«–
        model = info['model']
        try:
            proba = model.predict_proba(X_eval_p)[:, 1]
        except:
            proba = model.predict(X_eval_p) # è‹¥æ¨¡å‹ä¸æ”¯æ´æ©Ÿç‡ï¼Œå›é€€åˆ°é¡åˆ¥
        
        try: auc_val = roc_auc_score(y_sub, proba)
        except: auc_val = np.nan

        # ==========================================
        # [æ–°å¢åŠŸèƒ½] å¤šé–¾å€¼è¨ºæ–· (Threshold Diagnostic)
        # ==========================================
        original_threshold = info['threshold']
        
        # å®šç¾©æˆ‘å€‘è¦æ¸¬è©¦çš„é–¾å€¼ï¼šåŒ…å«åŸæœ¬çš„ï¼Œä»¥åŠè¼ƒé«˜çš„å¹¾å€‹é¸é …
        test_candidates = [original_threshold, 0.4, 0.5, 0.6, 0.7]
        test_thresholds = sorted(list(set(test_candidates))) # æ’åºä¸¦å»é‡
        
        print(f"   ğŸ“Š é–¾å€¼æ•æ„Ÿåº¦æ¸¬è©¦ (åŸæœ¬ Th={original_threshold:.3f})...")
        
        final_stats = {} # ç”¨ä¾†å­˜åŸæœ¬é–¾å€¼çš„çµæœ (å¯«å…¥ Excel ç”¨)

        for th in test_thresholds:
            pred_th = (proba >= th).astype(int)
            
            # è¨ˆç®—æŒ‡æ¨™
            f1_th = f1_score(y_sub, pred_th)
            acc_th = accuracy_score(y_sub, pred_th)
            prec_th = precision_score(y_sub, pred_th, zero_division=0)
            rec_th = recall_score(y_sub, pred_th, zero_division=0)
            
            cm_th = confusion_matrix(y_sub, pred_th, labels=[0, 1])
            tn, fp, fn, tp = cm_th.ravel() if cm_th.size==4 else (0,0,0,0)
            spec_th = tn/(tn+fp) if (tn+fp)>0 else 0
            npv_th = tn/(tn+fn) if (tn+fn)>0 else 0
            
            # æ¨™è¨˜å“ªä¸€å€‹æ˜¯åŸæœ¬çš„æ¨¡å‹è¨­å®š
            is_original = abs(th - original_threshold) < 1e-9
            marker = "â­" if is_original else "  "
            
            print(f"      {marker} Th={th:.2f} | F1={f1_th:.4f} | Recall={rec_th:.4f} | Spec={spec_th:.4f} | Acc={acc_th:.4f}")
            
            # å¦‚æœæ˜¯åŸæœ¬çš„é–¾å€¼ï¼Œæš«å­˜èµ·ä¾†ä¾›å¾ŒçºŒå„²å­˜
            if is_original:
                final_stats = {
                    "pred": pred_th, "f1": f1_th, "acc": acc_th, 
                    "prec": prec_th, "rec": rec_th, "spec": spec_th, "npv": npv_th,
                    "threshold": th
                }
        
        # ç¢ºä¿æœ‰å­˜åˆ°çµæœ (ä»¥é˜²æµ®é»æ•¸èª¤å·®ï¼Œè‹¥æ²’å°æ‡‰åˆ°å°±ç”¨æœ€å¾Œä¸€å€‹æˆ–åŸæœ¬çš„)
        if not final_stats:
             # Fallback: ä½¿ç”¨åŸæœ¬é–¾å€¼å†ç®—ä¸€æ¬¡
             pred = (proba >= original_threshold).astype(int)
             cm = confusion_matrix(y_sub, pred, labels=[0, 1])
             tn, fp, fn, tp = cm.ravel()
             final_stats = {
                "pred": pred,
                "f1": f1_score(y_sub, pred),
                "acc": accuracy_score(y_sub, pred),
                "prec": precision_score(y_sub, pred, zero_division=0),
                "rec": recall_score(y_sub, pred, zero_division=0),
                "spec": tn/(tn+fp) if (tn+fp)>0 else 0,
                "npv": tn/(tn+fn) if (tn+fn)>0 else 0,
                "threshold": original_threshold
             }

        # 7. å„²å­˜çµæœ (ä½¿ç”¨åŸæœ¬é–¾å€¼çš„è¡¨ç¾)
        results.append({
            "Label": label, 
            "F1": final_stats['f1'], "Acc": final_stats['acc'], "AUC": auc_val, 
            "Spec": final_stats['spec'], "NPV": final_stats['npv'], 
            "Prec": final_stats['prec'], "Recall": final_stats['rec'],
            "Threshold": final_stats['threshold']
        })
        
        # 8. é©—è­‰é›†ç¹ªåœ– (ä½¿ç”¨åŸæœ¬é–¾å€¼çš„é æ¸¬)
        viz = Visualizer(label, val_out_dir, sub_folder=label)
        viz.plot_confusion_matrix_aggregated(y_sub, final_stats['pred'])
        if len(np.unique(y_sub)) > 1:
            fpr, tpr, _ = roc_curve(y_sub, proba)
            viz.plot_roc_curve_with_ci([tpr], fpr, [auc_val])
            
    if results:
        res_df = pd.DataFrame(results)
        excel_path = os.path.join(val_out_dir, "External_Validation_Results.xlsx")
        res_df.to_excel(excel_path, index=False)
        
        cols = ["Label", "F1", "Prec", "Recall", "Spec", "NPV", "AUC", "Acc", "Threshold"]
        
        pretty_print_table(res_df[cols], title="å¤–éƒ¨é©—è­‰çµæœæ‘˜è¦ (Original Threshold)")
        print(f"\nå¤–éƒ¨é©—è­‰å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³: {val_out_dir}")
    else:
        print("\nç„¡æ³•ç”¢ç”Ÿä»»ä½•é©—è­‰çµæœ (å¯èƒ½ç¼ºè³‡æ–™æˆ–æ¨¡å‹)")