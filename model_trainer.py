import numpy as np
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV, cross_val_predict, StratifiedKFold
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score, recall_score, confusion_matrix, make_scorer
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN
import warnings

# å¿½ç•¥æ”¶æ–‚è­¦å‘Šèˆ‡åƒæ•¸è­¦å‘Š
warnings.filterwarnings('ignore')

class ModelTrainer:
    def __init__(self, label_name, pos_count, neg_count, current_f1, target_f1, use_stacking=False):
        self.label_name = label_name
        self.pos_count = pos_count
        self.neg_count = neg_count
        self.ratio = neg_count / pos_count if pos_count > 0 else 1
        self.target_f1 = target_f1
        self.use_stacking = use_stacking
        
        # ç­–ç•¥åˆ¤å®šï¼šé€™ç¾åœ¨å½±éŸ¿çš„æ˜¯ã€Œæœç´¢æ¬¡æ•¸ã€èˆ‡ã€Œæœç´¢ç¯„åœã€ï¼Œè€Œéå¯«æ­»åƒæ•¸
        self.gap = target_f1 - current_f1
        if self.gap > 0.10: self.strategy = 'aggressive'
        elif self.gap > 0.05: self.strategy = 'moderate'
        else: self.strategy = 'conservative'
        
        self.models = {}
        self.results = {}
        self.fitted_models = {}
        self.best_thresholds = {} # å„²å­˜æ¯å€‹æ¨¡å‹åœ¨è¨“ç·´é›†ä¸Šæ‰¾åˆ°çš„æœ€ä½³é–¾å€¼

    def get_sampling_strategy(self):
        """
        ä¿ç•™åŸæœ¬çš„æ¡æ¨£ç­–ç•¥é‚è¼¯ï¼Œé€™éƒ¨åˆ†ä¸æ¶‰åŠæ•¸æ“šæ´©æ¼ï¼Œå¯ç¹¼çºŒä½¿ç”¨ã€‚
        """
        if self.label_name == 'GAD': return 'BorderlineSMOTE', 0.35, 4
        if self.label_name == 'SSD': return 'BorderlineSMOTE', 0.40, 5
        if self.label_name == 'MDD': return 'SMOTE', 0.65, 5
        if self.label_name == 'Panic': return 'BorderlineSMOTE', 0.55, 4
        
        if self.pos_count < 100:
            sampler_type = 'ADASYN'
            ratio = 0.65 if self.strategy == 'aggressive' else 0.55
            k = 4
        else:
            sampler_type = 'SMOTE'
            ratio = 0.65 if self.strategy == 'aggressive' else 0.50
            k = 5
        return sampler_type, ratio, k

    def get_param_grid(self, model_name, scale_weight):
        """
        [New] å®šç¾©åƒæ•¸æœç´¢ç©ºé–“ï¼Œå–ä»£ç¡¬ç·¨ç¢¼ã€‚
        æ ¹æ“š strategy èª¿æ•´æœç´¢ç¯„åœã€‚
        """
        # åŸºç¤æ¬Šé‡è¨­å®š
        final_weight = max(1, int(scale_weight))

        if model_name == 'XGB':
            return {
                'n_estimators': [300, 500, 700, 900],
                'max_depth': [6, 10, 15, 20],
                'learning_rate': [0.01, 0.02, 0.05, 0.1],
                'scale_pos_weight': [final_weight, final_weight * 1.5],
                'subsample': [0.7, 0.8, 0.9],
                'colsample_bytree': [0.7, 0.8],
                'gamma': [0, 0.2, 0.5],
                'min_child_weight': [1, 3, 5],
                'reg_alpha': [0.1, 0.5, 1.0],
                'reg_lambda': [1.0, 1.5, 2.0]
            }
        elif model_name == 'LGBM':
            return {
                'n_estimators': [300, 500, 700, 900],
                'max_depth': [-1, 10, 20],
                'learning_rate': [0.01, 0.03, 0.05],
                'num_leaves': [31, 50, 80],
                'class_weight': [{0:1, 1:final_weight}, {0:1, 1:final_weight*1.2}],
                'subsample': [0.8, 0.9],
                'reg_alpha': [0.1, 0.5],
                'reg_lambda': [0.5, 1.0]
            }
        elif model_name in ['RF', 'ET']:
            return {
                'n_estimators': [300, 500, 800],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'class_weight': [{0:1, 1:final_weight}, 'balanced']
            }
        elif model_name == 'GB':
            return {
                'n_estimators': [200, 400, 600],
                'max_depth': [3, 5, 8],
                'learning_rate': [0.01, 0.05, 0.1],
                'subsample': [0.8, 0.9]
            }
        elif model_name == 'BalancedRF':
            return {
                'n_estimators': [300, 500, 800],
                'max_depth': [None, 10, 20],
                'min_samples_leaf': [2, 4]
            }
        return {}

    def build_models(self):
        """
        åˆå§‹åŒ–åŸºç¤æ¨¡å‹ç‰©ä»¶ï¼Œé€™è£¡ä¸å†è¨­å®šè©³ç´°åƒæ•¸ï¼Œè©³ç´°åƒæ•¸äº¤ç”± Tuning æ±ºå®šã€‚
        """
        self.models['XGB'] = xgb.XGBClassifier(n_jobs=-1, verbosity=0, random_state=42, use_label_encoder=False)
        self.models['LGBM'] = lgb.LGBMClassifier(n_jobs=-1, verbose=-1, random_state=42)
        self.models['RF'] = RandomForestClassifier(n_jobs=-1, random_state=42)
        self.models['ET'] = ExtraTreesClassifier(n_jobs=-1, random_state=42)
        self.models['GB'] = GradientBoostingClassifier(random_state=42)
        self.models['BalancedRF'] = BalancedRandomForestClassifier(n_jobs=-1, random_state=42)

    def _tune_and_fit(self, name, model, X_res, y_res):
        """
        [New] è‡ªå‹•åƒæ•¸æœç´¢èˆ‡è¨“ç·´
        """
        scale_weight = self.ratio
        param_dist = self.get_param_grid(name, scale_weight)
        
        # æ ¹æ“šç­–ç•¥æ±ºå®šæœç´¢è¿­ä»£æ¬¡æ•¸ (n_iter)
        # Aggressive = æ›´å¤šæ¬¡å˜—è©¦
        n_iter = 10 if self.strategy == 'aggressive' else 5
        if self.strategy == 'conservative': n_iter = 5
        
        # é‡å°ç‰¹å®šæ¨¡å‹çš„å°å„ªåŒ–ï¼Œè‹¥åƒæ•¸ç¶²æ ¼å¾ˆå°å‰‡æ¸›å°‘è¿­ä»£
        if not param_dist:
            model.fit(X_res, y_res)
            return model

        print(f"      ğŸ”§ Tuning {name} (iter={n_iter})...", end="\r")
        
        # ä½¿ç”¨ F1 ä½œç‚ºå„ªåŒ–ç›®æ¨™
        scorer = make_scorer(f1_score)
        
        search = RandomizedSearchCV(
            estimator=model,
            param_distributions=param_dist,
            n_iter=n_iter,
            scoring=scorer,
            cv=3, # 3-Fold å…§éƒ¨é©—è­‰æ‰¾åƒæ•¸
            n_jobs=-1,
            random_state=42,
            verbose=0
        )
        
        try:
            search.fit(X_res, y_res)
            best_model = search.best_estimator_
            # print(f"      âœ… {name} Tuned. Best F1: {search.best_score_:.3f}")
            return best_model
        except Exception as e:
            print(f"      âš ï¸ Tuning failed for {name}: {e}, using default.")
            model.fit(X_res, y_res)
            return model

    # def _optimize_threshold(self, y_true, y_pred_proba):
    #     """
    #     å°‹æ‰¾æœ€ä½³é–¾å€¼ã€‚
    #     æ³¨æ„ï¼šé€™å€‹å‡½æ•¸ç¾åœ¨åªè¢«ç”¨æ–¼ Training Set (OOF predictions) ä¸Šã€‚
    #     """
    #     thresholds = np.linspace(0.10, 0.90, 100)
        
    #     # ä¿ç•™åŸæœ¬çš„ Precision/Recall é™åˆ¶é‚è¼¯
    #     if self.label_name == 'GAD': min_prec, min_rec = 0.62, 0.60
    #     elif self.label_name == 'SSD': min_prec, min_rec = 0.68, 0.60
    #     elif self.label_name == 'MDD': min_prec, min_rec = 0.70, 0.60
    #     elif self.label_name == 'Panic': min_prec, min_rec = 0.45, 0.45
    #     else: min_prec, min_rec = 0.50, 0.30

    #     best_f1, best_thresh = 0, 0.5
        
    #     for t in thresholds:
    #         pred = (y_pred_proba >= t).astype(int)
    #         if pred.sum() == 0: continue
    #         p = precision_score(y_true, pred, zero_division=0)
    #         r = recall_score(y_true, pred, zero_division=0)
            
    #         # åªæœ‰æ»¿è¶³æœ€å° P/R è¦æ±‚æ‰è€ƒæ…®è©² F1
    #         if p >= min_prec and r >= min_rec:
    #             f1 = f1_score(y_true, pred)
    #             if f1 > best_f1: best_f1, best_thresh = f1, t
        
    #     # Fallback: å¦‚æœæ²’æœ‰é–¾å€¼æ»¿è¶³æ¢ä»¶ï¼Œå°±æ‰¾å–®ç´” F1 æœ€é«˜çš„
    #     if best_f1 == 0: 
    #         for t in thresholds:
    #             pred = (y_pred_proba >= t).astype(int)
    #             if pred.sum() == 0: continue
    #             f1 = f1_score(y_true, pred)
    #             if f1 > best_f1: best_f1, best_thresh = f1, t
                
    #     return best_thresh

    def _optimize_threshold(self, y_true, y_pred_proba):
        """
        [Modified] å°‹æ‰¾æœ€ä½³é–¾å€¼ï¼šä½¿ç”¨ Youden's Index æˆ– F1ï¼Œä½†å¼·åˆ¶è¦æ±‚æœ€å° Specificityã€‚
        """
        thresholds = np.linspace(0.20, 0.80, 100) # ç¯„åœç¸®å°ï¼Œé¿å…æ¥µç«¯å€¼
        
        # 1. è¨­å®šåš´æ ¼çš„é™åˆ¶æ¢ä»¶
        # å°æ–¼ GADï¼Œæˆ‘å€‘å¸Œæœ› Specificity è‡³å°‘è¦åŠæ ¼ (ä¾‹å¦‚ > 0.6)
        if self.label_name == 'GAD': 
            min_prec, min_rec, min_spec = 0.50, 0.60, 0.60
        elif self.label_name == 'Panic':
            min_prec, min_rec, min_spec = 0.40, 0.50, 0.70 # Panic èª¤åˆ¤ç‡å¤ªé«˜ï¼Œéœ€æé«˜ Spec è¦æ±‚
        else: 
            min_prec, min_rec, min_spec = 0.50, 0.60, 0.50

        best_score = -1
        best_thresh = 0.5
        
        for t in thresholds:
            pred = (y_pred_proba >= t).astype(int)
            if pred.sum() == 0: continue # é¿å…å…¨ 0
            
            # è¨ˆç®—å„é …æŒ‡æ¨™
            tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0, 1]).ravel()
            spec = tn / (tn + fp) if (tn + fp) > 0 else 0
            rec = tp / (tp + fn) if (tp + fn) > 0 else 0
            prec = tp / (tp + fp) if (tp + fp) > 0 else 0
            
            # åªæœ‰ç•¶ Recall å’Œ Specificity éƒ½æ»¿è¶³åº•ç·šæ™‚ï¼Œæ‰è€ƒæ…®é€™å€‹é–¾å€¼
            if prec >= min_prec and rec >= min_rec and spec >= min_spec:
                # é€™è£¡æ”¹ç”¨ F1 * Specificity ä½œç‚ºç¶œåˆåˆ†æ•¸ï¼Œé¼“å‹µå…©è€…çš†é«˜
                # æˆ–è€…ä½¿ç”¨ Youden's Index: score = rec + spec - 1
                score = f1_score(y_true, pred)
                
                if score > best_score:
                    best_score = score
                    best_thresh = t
        
        # Fallback: å¦‚æœå¤ªåš´æ ¼å°è‡´æ‰¾ä¸åˆ°é–¾å€¼ï¼Œé€€å›åˆ°å°‹æ‰¾ Youden's Index æœ€å¤§å€¼
        if best_score == -1:
            best_j = -1
            for t in thresholds:
                pred = (y_pred_proba >= t).astype(int)
                tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0, 1]).ravel()
                spec = tn / (tn + fp) if (tn + fp) > 0 else 0
                rec = tp / (tp + fn) if (tp + fn) > 0 else 0
                
                # Youden's J = Sensitivity + Specificity - 1
                j_index = rec + spec - 1
                if j_index > best_j:
                    best_j = j_index
                    best_thresh = t
            print(f"      âš ï¸ Fallback to Youden's Index for {self.label_name}, Th={best_thresh:.2f}")

        return best_thresh

    def _find_best_threshold_via_cv(self, model, X_train, y_train):
        """
        [New] [Priority 1 Solution]
        åœ¨è¨“ç·´é›†ä¸Šä½¿ç”¨ Cross-Validation (OOF) é æ¸¬ä¾†å°‹æ‰¾æœ€ä½³é–¾å€¼ã€‚
        é€™è§£æ±ºäº† Data Leakage å•é¡Œã€‚
        """
        try:
            # å–å¾— Out-of-Fold é æ¸¬æ©Ÿç‡
            # cv=3 è¡¨ç¤ºå°‡è¨“ç·´é›†åˆ‡æˆ3ä»½ï¼Œè¼ªæµé æ¸¬ï¼Œç¢ºä¿æ¯å€‹æ¨£æœ¬éƒ½æ˜¯åœ¨æœªçœ‹éè©²æ¨£æœ¬çš„æ¨¡å‹ä¸‹é æ¸¬çš„
            oof_proba = cross_val_predict(model, X_train, y_train, cv=3, method='predict_proba')[:, 1]
            
            # åœ¨é€™äº› OOF æ©Ÿç‡ä¸Šæ‰¾æœ€ä½³é–¾å€¼
            best_thresh = self._optimize_threshold(y_train, oof_proba)
            return best_thresh
        except Exception as e:
            print(f"      âš ï¸ CV Thresholding failed: {e}, using 0.5")
            return 0.5

    def _create_stacking(self, X_train, X_test, y_train, y_test):
        if len(self.fitted_models) < 2: return
        
        train_meta = []
        test_meta = []
        
        # å°æ–¼ Stackingï¼Œè¨“ç·´é›†çš„ Meta Feature ä¹Ÿæ‡‰è©²æ˜¯ OOF é æ¸¬ï¼Œå¦å‰‡æœƒéæ“¬åˆ
        # ä½†ç‚ºäº†ç°¡åŒ–é‹ç®—ä¸”ä¿ç•™æ‚¨çš„åŸå§‹çµæ§‹ï¼Œé€™è£¡æˆ‘å€‘ä½¿ç”¨å·²è¨“ç·´å¥½çš„æ¨¡å‹
        # *æ”¹é€²*: ç†è«–ä¸Šé€™è£¡ä¹Ÿè©²ç”¨ cross_val_predictï¼Œä½†è¨ˆç®—é‡æœƒè®Šå¤§ã€‚
        # é€™è£¡æˆ‘å€‘ç¶­æŒç°¡å–®ï¼Œä½†ä½¿ç”¨é‡å° Training Set æ‰¾å‡ºçš„ Threshold é‚è¼¯
        
        valid_models = []
        for name, model in self.fitted_models.items():
            if not hasattr(model, "predict_proba"): continue
            valid_models.append(model)
            train_meta.append(model.predict_proba(X_train)[:, 1])
            test_meta.append(model.predict_proba(X_test)[:, 1])
            
        if not train_meta: return
        
        meta_X_train = np.vstack(train_meta).T
        meta_X_test  = np.vstack(test_meta).T
        
        cw = {0:1.0, 1:(self.neg_count/self.pos_count)} if self.pos_count>0 else None
        meta_clf = LogisticRegression(max_iter=1000, class_weight=cw, random_state=42)
        
        # 1. è¨“ç·´ Meta Learner
        meta_clf.fit(meta_X_train, y_train)
        
        # 2. å°‹æ‰¾ Meta Learner çš„é–¾å€¼ (ä½¿ç”¨ OOF é¿å…æ´©æ¼)
        meta_thresh = self._find_best_threshold_via_cv(meta_clf, meta_X_train, y_train)
        
        # 3. é æ¸¬æ¸¬è©¦é›†
        stack_proba = meta_clf.predict_proba(meta_X_test)[:, 1]
        pred = (stack_proba >= meta_thresh).astype(int)
        
        self._save_result('Stacking', y_test, pred, stack_proba, meta_thresh, meta_clf)

    def _create_top3_ensemble(self, X_test, y_test):
        # é€™è£¡ä¸éœ€è¦è¨“ç·´ï¼Œåªéœ€å¹³å‡æ¦‚ç‡
        base_res = {k:v for k,v in self.results.items() if k not in ['Stacking', 'Ensemble']}
        if len(base_res) < 2: return
        
        # é¸æœ€å¥½çš„å‰ä¸‰å€‹
        sorted_models = sorted(base_res.items(), key=lambda x: x[1]['f1_score'], reverse=True)
        top_models = sorted_models[:3]
        
        # å–å‡ºé€™ä¸‰å€‹æ¨¡å‹çš„åç¨±
        top_names = [m[0] for m in top_models]
        
        # è¨ˆç®—å¹³å‡æ©Ÿç‡
        preds = [m[1]['y_pred_proba'] for m in top_models]
        ens_proba = np.mean(preds, axis=0)
        
        # [Crucial] Ensemble çš„é–¾å€¼è©²æ€éº¼å®šï¼Ÿ
        # æ‡‰è©²æ˜¯é€™ä¸‰å€‹æ¨¡å‹åœ¨è¨“ç·´é›†ä¸Šçš„æœ€ä½³é–¾å€¼çš„å¹³å‡ï¼Œæˆ–æ˜¯é‡æ–°è¨ˆç®—ï¼Ÿ
        # ç°¡å–®èµ·è¦‹ï¼Œæˆ‘å€‘å–ä¸‰å€‹æ¨¡å‹é–¾å€¼çš„å¹³å‡
        avg_thresh = np.mean([self.best_thresholds[name] for name in top_names])
        
        pred = (ens_proba >= avg_thresh).astype(int)
        
        self._save_result('Ensemble', y_test, pred, ens_proba, avg_thresh, None)

    def _save_result(self, name, y_true, y_pred, y_proba, thresh, model):
        f1 = f1_score(y_true, y_pred)
        acc = accuracy_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred, zero_division=0)
        rec = recall_score(y_true, y_pred, zero_division=0)
        try: auc = roc_auc_score(y_true, y_proba)
        except: auc = np.nan
        
        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
        tn, fp, fn, tp = cm.ravel() if cm.size==4 else (0,0,0,0)
        spec = tn/(tn+fp) if (tn+fp)>0 else 0
        npv = tn/(tn+fn) if (tn+fn)>0 else 0
        
        self.results[name] = {
            'f1_score': f1, 'accuracy': acc, 'auc': auc,
            'precision': prec, 'recall': rec, 'specificity': spec, 'npv': npv,
            'threshold': thresh, 'y_pred': y_pred, 'y_pred_proba': y_proba, 
            'y_true': y_true.values, 'model': model,
            'shap_values': None, 'feature_importance': None
        }
        status = "âœ…" if f1 >= self.target_f1 else "  "
        print(f"      {name:12s}: F1={f1:.4f} {status}, P={prec:.3f}, R={rec:.3f}, Spec={spec:.3f} (Th={thresh:.2f})")

    def train_and_evaluate(self, X_train, X_test, y_train, y_test):
        """
        ä¸»æµç¨‹ï¼šæ¡æ¨£ -> Tuning -> æ‰¾é–¾å€¼(Train) -> é æ¸¬(Test)
        """
        stype, sratio, k = self.get_sampling_strategy()
        
        # æ¡æ¨£æª¢æŸ¥
        n_pos = int(y_train.sum())
        n_neg = len(y_train) - n_pos
        curr_ratio = n_pos/n_neg if n_neg>0 else 1
        
        if curr_ratio >= sratio:
            sratio = min(curr_ratio + 0.15, 1.0)
        
        try:
            if stype == 'ADASYN': sampler = ADASYN(sampling_strategy=sratio, n_neighbors=k, random_state=42)
            elif stype == 'BorderlineSMOTE': sampler = BorderlineSMOTE(sampling_strategy=sratio, k_neighbors=k, random_state=42)
            else: sampler = SMOTE(sampling_strategy=sratio, k_neighbors=k, random_state=42)
            X_res, y_res = sampler.fit_resample(X_train, y_train)
        except:
            X_res, y_res = X_train, y_train
            
        print(f"      [Data] Train: {len(X_train)} -> Resampled: {len(X_res)}")
            
        for name, model in self.models.items():
            # 1. è‡ªå‹•åƒæ•¸èª¿æ•´ (Tuning) + æ“¬åˆ (Fitting)
            # æ³¨æ„ï¼šé€™è£¡æ˜¯åœ¨ Resampled æ•¸æ“šä¸Šé€²è¡Œ Tuning å’Œ Fitting
            fitted_model = self._tune_and_fit(name, model, X_res, y_res)
            self.fitted_models[name] = fitted_model
            
            # 2. [Priority 1 Fix] åœ¨è¨“ç·´é›†ä¸Šå°‹æ‰¾æœ€ä½³é–¾å€¼ (ä½¿ç”¨ Cross-Validation é¿å…æ´©æ¼)
            # æˆ‘å€‘ä½¿ç”¨ Resampled æ•¸æ“šä¾†æ‰¾é–¾å€¼ï¼Œå› ç‚ºæ¨¡å‹æ˜¯åœ¨é€™ç¨®åˆ†ä½ˆä¸Šè¨“ç·´çš„
            best_thresh = self._find_best_threshold_via_cv(fitted_model, X_res, y_res)
            self.best_thresholds[name] = best_thresh
            
            # 3. åœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œé æ¸¬ (ä½¿ç”¨å‰›å‰›æ‰¾åˆ°çš„é–¾å€¼)
            proba = fitted_model.predict_proba(X_test)[:, 1]
            # åš´æ ¼ç¦æ­¢åœ¨é€™è£¡ä½¿ç”¨ y_test ä¾†æ‰¾é–¾å€¼ï¼ç›´æ¥ä½¿ç”¨ best_thresh
            pred = (proba >= best_thresh).astype(int)
            
            self._save_result(name, y_test, pred, proba, best_thresh, fitted_model)

        if self.use_stacking:
            self._create_stacking(X_res, X_test, y_res, y_test)
            self._create_top3_ensemble(X_test, y_test)
            
        return self.results